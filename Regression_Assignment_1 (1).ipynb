{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42b020f8-e9d0-415d-8f39-065f0bf13716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is example of simple linear regression\n",
      "   X  Y\n",
      "0  0  1\n",
      "1  1  3\n",
      "2  2  2\n",
      "3  3  5\n",
      "4  4  7\n",
      "r2_score:  0.45729681888868934\n"
     ]
    }
   ],
   "source": [
    "# Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "# Answer: \n",
    "# SIMPLE LINEAR REGRESSION: Single linear regression is an approach for predicting a responce using a single feature.\n",
    "# MULTILINEAR REGRESSION: Multilinear regression attempts to model the relationship two or more features and a responce by \n",
    "#                          fitting a linear equation to observ data\n",
    "# EXAMPLE OF SIMPLE LINEAR REGRESSION:\n",
    "print(\"This is example of simple linear regression\")\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X,y = load_diabetes(return_X_y=True)\n",
    "print(df.head())\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)\n",
    "LR = LinearRegression()\n",
    "LR.fit(X,y)\n",
    "y_pred = LR.predict(X_test)\n",
    "print(\"r2_score: \",r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3766ac3e-d856-4d01-ae3e-5b4de66999b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the example of multilinear regression\n",
      "   feature1  feature2      target\n",
      "0 -0.097988  1.840329  163.517040\n",
      "1  0.454925 -1.175883 -109.140363\n",
      "2 -0.194257 -0.674744   13.378484\n",
      "3 -1.914498  1.879115   86.174331\n",
      "4  0.264669  0.238681  -17.091595\n",
      "r2_score:  0.7247854770844526\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the example of multilinear regression\")\n",
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X,y = make_regression(n_samples=100, n_features=2, n_informative=2, n_targets=1, noise=50)\n",
    "df = pd.DataFrame({'feature1':X[:,0],'feature2':X[:,1],'target':y})\n",
    "print(df.head())\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=3)\n",
    "MLR = LinearRegression()\n",
    "MLR.fit(X_train,y_train)\n",
    "y_pred = MLR.predict(X_test)\n",
    "print(\"r2_score: \", r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0b08b-40ef-4890-96dc-a53067c5eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "# Answer:\n",
    "# Linearity: This assumption states that the relationship between the independent and dependent variables is linear. You can check this assumption by \n",
    "# creating scatterplots of the variables and looking for a roughly linear pattern. If the data points cluster around a straight line, the linearity assumption \n",
    "# is likely met. Additionally, residual plots can be used to identify deviations from linearity.\n",
    "\n",
    "# Independence of Errors: This assumption assumes that the errors (residuals) are independent of each other. In other words, the error for one data point\n",
    "# should not be correlated with the error for another data point. You can check this assumption using autocorrelation plots of the residuals or by examining\n",
    "# the residuals over time (if time series data).\n",
    "\n",
    "# Homoscedasticity: Homoscedasticity means that the variance of the residuals is constant across all levels of the independent variables. To check this assumption,\n",
    "# you can plot the residuals against the predicted values or the independent variables. Look for a consistent spread of points around zero. Heteroscedasticity, \n",
    "# where the spread of residuals changes as you move along the regression line, may indicate a violation of this assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2afda6b6-a2ea-4af4-b688-e7483eb5cbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is coefficients of best fit line:  [ 9.42798533 57.77994522]\n",
      "This is the intercept value of best fit line:  5.367816120554051\n"
     ]
    }
   ],
   "source": [
    "# Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "print(\"This is coefficients of best fit line: \",MLR.coef_)\n",
    "print(\"This is the intercept value of best fit line: \",MLR.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "913371f9-6fe2-458a-9b4a-aa9dc0f8417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "# Answer:\n",
    "# GRADIENT DESCENT: gradient descent is a first order derivative interative optimization algorithm for finding a local minimum\n",
    "# of a differential function. It is update the value of coefficients after certain time\n",
    "# Gradient Descent is three type:\n",
    "# (1.) Batch Gradient Descent\n",
    "# (2.) Stochastic Gradient Descent\n",
    "# (3.) Mini Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e347d95a-498c-41de-9a31-2f1b6eb978a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is example of simple linear regression\n",
      "   feature1  feature2      target\n",
      "0 -0.097988  1.840329  163.517040\n",
      "1  0.454925 -1.175883 -109.140363\n",
      "2 -0.194257 -0.674744   13.378484\n",
      "3 -1.914498  1.879115   86.174331\n",
      "4  0.264669  0.238681  -17.091595\n",
      "r2_score:  0.45729681888868934\n"
     ]
    }
   ],
   "source": [
    "# Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "print(\"This is example of simple linear regression\")\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X,y = load_diabetes(return_X_y=True)\n",
    "print(df.head())\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)\n",
    "LR = LinearRegression()\n",
    "LR.fit(X,y)\n",
    "y_pred = LR.predict(X_test)\n",
    "print(\"r2_score: \",r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8b7bc22-a3f6-4f33-b2fc-e13d815af3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the example of multilinear regression\n",
      "   feature1  feature2     target\n",
      "0 -1.889580  1.675428 -43.964104\n",
      "1 -1.599306 -0.530268  34.024509\n",
      "2 -1.363637 -0.388496  20.659412\n",
      "3 -0.982470 -0.420940 -71.654491\n",
      "4  0.526847  1.687696   1.440900\n",
      "r2_score:  0.48462179356965907\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the example of multilinear regression\")\n",
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X,y = make_regression(n_samples=100, n_features=2, n_informative=2, n_targets=1, noise=50)\n",
    "df = pd.DataFrame({'feature1':X[:,0],'feature2':X[:,1],'target':y})\n",
    "print(df.head())\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=3)\n",
    "MLR = LinearRegression()\n",
    "MLR.fit(X_train,y_train)\n",
    "y_pred = MLR.predict(X_test)\n",
    "print(\"r2_score: \", r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8079c33f-2b7c-4da8-968a-1b9253bb8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "# Multicollinearity: It generally occurs when the independent variables in a regression model are correlated with each other.\n",
    "# This correlation is not expected as the independent variables are assumed to be independent. If the degree of this \n",
    "# correlation is high, it may cause problems while predicting results from the model.\n",
    "\n",
    "# Variance Inflating factor (VIF) is used to test the presence of multicollinearity in a regression model.\n",
    "# 1 => not correlated. Multicollinearity doesnâ€™t exist.\n",
    "# Between 1 and 5 => moderately correlated. Low multicollinearity exists.\n",
    "# Greater than 5 => Highly correlated. High Multicollinearity exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a88e8ab0-95c4-4eac-b909-34c74b905e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "# Polynomial Regression is a form of linear regression in which the relationship between the independent variable x and \n",
    "# dependent variable y is modeled as an nth-degree polynomial. Polynomial regression fits a nonlinear relationship between the \n",
    "# value of x and the corresponding conditional mean of y, denoted E(y | x).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d87f47d-29a8-4b3f-bd01-27324d8458ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations \n",
    "#   would you prefer to use polynomial regression?\n",
    "# Answer:\n",
    "# Polynomial regression is a type of regression analysis that extends linear regression by fitting a polynomial equation to the data, \n",
    "# allowing for more complex relationships between the independent and dependent variables. Here are some advantages and disadvantages of \n",
    "# polynomial regression compared to linear regression:\n",
    "\n",
    "# **Advantages of Polynomial Regression:**\n",
    "# 1. **Captures Non-Linear Relationships:** Polynomial regression can model non-linear relationships between variables more effectively than linear regression. \n",
    "#     This allows you to account for curvature and better fit data that doesn't follow a straight line.\n",
    "# 2. **Flexibility:** Polynomial regression allows for greater flexibility in modeling data with curves and turns. You can choose the degree of the polynomial\n",
    "#    (e.g., quadratic, cubic) to fit the complexity of the data.\n",
    "# 3. **Improved Fit:** In cases where a linear model does not adequately capture the underlying data patterns, polynomial regression can result in a \n",
    "#    significantly improved fit and better predictive performance.\n",
    "# **Disadvantages of Polynomial Regression:**\n",
    "# 1. **Overfitting:** One of the main disadvantages of polynomial regression is that it is prone to overfitting, especially with higher-degree polynomials.\n",
    "#    Using a high-degree polynomial can lead to a model that fits the training data very well but generalizes poorly to new, unseen data.\n",
    "# 2. **Complexity:** As the degree of the polynomial increases, the model becomes more complex and harder to interpret. This can make it challenging to \n",
    "#    identify the actual relationship between variables.\n",
    "# 3. **Data Sensitivity:** Polynomial regression is sensitive to outliers, and small changes in data can lead to significant changes in the resulting model. \n",
    "#    It can be less robust in the presence of noisy or irregular data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
